## ğŸ’¬ About This Project

**Ollama Chat UI** is a lightweight and responsive chat interface built for interacting with **Ollama's local (offline) language models**. It's designed to provide a clean, user-friendly frontend for running LLMs on your own machineâ€”no cloud or API keys required.

### âœ¨ Key Features

- ğŸ§  **Offline AI Chat**: Connects directly to locally running Ollama modelsâ€”no internet or external API required.
- âš™ï¸ **Customizable Settings**: Easily change model parameters such as:
  - Selected model (e.g., `llama3`, `mistral`, etc.)
  - Temperature
  - Max tokens
  - System prompts
- ğŸ“‚ **Simple Setup**: No complex configuration. Just run Ollama locally and start chatting.
- ğŸ–¥ï¸ **Built for Developers and Enthusiasts**: Ideal for testing, local LLM experimentation, and integrating with offline AI workflows.


## ğŸ“¸ Demo

![Chat UI Demo](assets/image1.png)  

## ğŸ› ï¸ Installation

### Prerequisites

Before you begin, make sure you have the following installed:

- [Node.js](https://nodejs.org/) (Version 20 or above)
- [npm](https://npmjs.com) (Package manager for Node.js)

### Steps to Install

1. Clone the repository:

   ```bash
   git clone https://github.com/Kirtanmojidra/OllamaChatUI.git
   cd OllamaChatUI
   ```
   
1. Install dependencies:

   ```bash
   npm install
   ```
1. Install dependencies:
   make sure Ollama is runing or if not running then run **ollama serve**
   ```bash
   npm run dev
   ```
   Running on Network and localhost check Terminal Output For More Details

## ğŸ¤ Contributing

We love contributions from the community! Whether it's fixing bugs, improving the documentation, or adding new features,
